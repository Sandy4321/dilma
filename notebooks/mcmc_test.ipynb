{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.iterators import BasicIterator\n",
    "from allennlp.nn.util import move_to_device\n",
    "\n",
    "from adat.utils import load_weights, calculate_wer\n",
    "from adat.masker import SimpleMasker, MASK_TOKEN\n",
    "from adat.models import get_basic_classification_model, get_basic_seq2seq_model\n",
    "from adat.dataset import InsuranceReader, OneLangSeq2SeqReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 10 21:51:31 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   26C    P8     9W / 280W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "|  0%   27C    P8     8W / 280W |     10MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:83:00.0 Off |                  N/A |\n",
      "|  0%   24C    P8     8W / 280W |    581MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:84:00.0 Off |                  N/A |\n",
      "|  0%   21C    P8     8W / 280W |  10633MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "cuda_device = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "min_length = 2\n",
    "\n",
    "data = pd.read_csv('../data/full.csv')\n",
    "\n",
    "data = data[['treatments', 'target']]\n",
    "treatment_len = data.treatments.apply(lambda x: len(x.split()))\n",
    "data = data[(treatment_len <= max_length) & (treatment_len >= min_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['seq_len'] = data.treatments.apply(lambda x: len(x.split()))\n",
    "\n",
    "negative = data[data.target == 0]\n",
    "positive = data[data.target == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_examples = defaultdict(list)\n",
    "positive_examples = defaultdict(list)\n",
    "\n",
    "for row in negative.itertuples():\n",
    "    negative_examples[row.seq_len].append(row.treatments.strip())\n",
    "    \n",
    "for row in positive.itertuples():\n",
    "    positive_examples[row.seq_len].append(row.treatments.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_reader = OneLangSeq2SeqReader(masker=None)\n",
    "seq2seq_vocab = Vocabulary.from_files('vocab_seq2seq_masked')\n",
    "seq2seq_model = get_basic_seq2seq_model(seq2seq_vocab)\n",
    "load_weights(seq2seq_model, 'model_seq2seq_masked.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_reader = InsuranceReader()\n",
    "class_vocab = Vocabulary.from_files('vocab_classification')\n",
    "class_model = get_basic_classification_model(class_vocab)\n",
    "load_weights(class_model, 'model_classification.th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adat.mcmc import MCMCSampler, NormalProposal\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_1737 a_1690 a_1690 a_1737 a_2001 a_1667\n"
     ]
    }
   ],
   "source": [
    "example = positive_examples[6][3]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal = NormalProposal()\n",
    "sampler = MCMCSampler(\n",
    "    proposal_distribution=proposal, \n",
    "    classification_model=class_model, \n",
    "    classification_reader=class_reader, \n",
    "    generation_model=seq2seq_model, \n",
    "    generation_reader=seq2seq_reader,\n",
    "    initial_sequence=example,\n",
    "    l2_norm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/allennlp/nn/beam_search.py:127: RuntimeWarning: Empty sequences predicted. You may want to increase the beam size or ensure your step function is working properly.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "history = sampler.sample(num_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.6900655593423543,\n",
      " 'bleu_diff': -0.30993444065764575,\n",
      " 'generated_sequence': 'a_1690 a_1690 a_1690 a_1737 a_2001 a_1667 a_1667',\n",
      " 'l2_norm': 2.3845975,\n",
      " 'prob': 0.22387059,\n",
      " 'prob_diff': 0.113484874}\n",
      "\n",
      "{'bleu': 0.816496580927726,\n",
      " 'bleu_diff': -0.18350341907227397,\n",
      " 'generated_sequence': 'a_1690 a_1690 a_1690 a_1737 a_2001 a_1667',\n",
      " 'l2_norm': 1.9094037,\n",
      " 'prob': 0.25439045,\n",
      " 'prob_diff': 0.14400473}\n",
      "\n",
      "{'bleu': 0.816496580927726,\n",
      " 'bleu_diff': -0.18350341907227397,\n",
      " 'generated_sequence': 'a_1690 a_1690 a_1690 a_1737 a_2001 a_1667',\n",
      " 'l2_norm': 1.6861911,\n",
      " 'prob': 0.25439045,\n",
      " 'prob_diff': 0.14400473}\n",
      "\n",
      "{'bleu': 0.816496580927726,\n",
      " 'bleu_diff': 0.0,\n",
      " 'generated_sequence': 'a_1731 a_1690 a_1690 a_1737 a_2001 a_1667',\n",
      " 'l2_norm': 2.0001087,\n",
      " 'prob': 0.120621905,\n",
      " 'prob_diff': 0.101751}\n",
      "\n",
      "{'bleu': 0.4364357804719847,\n",
      " 'bleu_diff': -0.40871847425653185,\n",
      " 'generated_sequence': 'a_375 a_1690 a_1690 a_1690 a_1737 a_1667 a_1667',\n",
      " 'l2_norm': 1.906865,\n",
      " 'prob': 0.25257355,\n",
      " 'prob_diff': 0.15695992}\n",
      "\n",
      "{'bleu': 0.6900655593423543,\n",
      " 'bleu_diff': -0.15508869538616232,\n",
      " 'generated_sequence': 'a_2054 a_1690 a_1690 a_1737 a_2001 a_1667 a_1667',\n",
      " 'l2_norm': 1.4510365,\n",
      " 'prob': 0.2880026,\n",
      " 'prob_diff': 0.19238898}\n",
      "\n",
      "{'bleu': 0.4364357804719847,\n",
      " 'bleu_diff': -0.2536297788703695,\n",
      " 'generated_sequence': 'a_1 a_1690 a_1690 a_1690 a_1737 a_1667 a_1667',\n",
      " 'l2_norm': 1.4195088,\n",
      " 'prob': 0.27653012,\n",
      " 'prob_diff': 0.18170533}\n",
      "\n",
      "{'bleu': 0.6900655593423543,\n",
      " 'bleu_diff': 0.0,\n",
      " 'generated_sequence': 'a_645 a_1690 a_1690 a_1737 a_2001 a_1667 a_1667',\n",
      " 'l2_norm': 1.7702851,\n",
      " 'prob': 0.21073677,\n",
      " 'prob_diff': 0.11591197}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ex in history:\n",
    "    if ex['prob_diff'] > 0.1:\n",
    "        pprint(ex)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
