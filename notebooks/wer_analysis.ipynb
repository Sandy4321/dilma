{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.iterators import BasicIterator\n",
    "from allennlp.nn.util import move_to_device\n",
    "\n",
    "from adat.utils import load_weights, calculate_wer\n",
    "from adat.masker import SimpleMasker, MASK_TOKEN\n",
    "from adat.models import get_basic_classification_model, get_basic_seq2seq_model\n",
    "from adat.dataset import InsuranceReader, OneLangSeq2SeqReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 25 22:24:46 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.00    Driver Version: 418.87.00    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 44%   55C    P2    63W / 280W |   1351MiB / 11178MiB |      9%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "|  0%   30C    P8     8W / 280W |  10776MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "|  0%   26C    P8     8W / 280W |  10776MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:83:00.0 Off |                  N/A |\n",
      "|  0%   27C    P8     9W / 280W |   2748MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "cuda_device = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "min_length = 2\n",
    "\n",
    "data = pd.read_csv('../data/full.csv')\n",
    "\n",
    "data = data[['treatments', 'target']]\n",
    "treatment_len = data.treatments.apply(lambda x: len(x.split()))\n",
    "data = data[(treatment_len <= max_length) & (treatment_len >= min_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treatments</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a_178 a_1884 a_1 a_168 a_172 a_174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a_1978 a_710 a_1677 a_1701 a_1 a_585 a_375 a_...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a_1257 a_545 a_1128 a_1 a_1191 a_2001 a_1978 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a_737 a_20 a_1257 a_1191 a_642 a_1 a_733 a_11...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a_719 a_1 a_347 a_340 a_1656 a_885 a_905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          treatments  target\n",
       "0                 a_178 a_1884 a_1 a_168 a_172 a_174       0\n",
       "2   a_1978 a_710 a_1677 a_1701 a_1 a_585 a_375 a_...       0\n",
       "3   a_1257 a_545 a_1128 a_1 a_1191 a_2001 a_1978 ...       0\n",
       "4   a_737 a_20 a_1257 a_1191 a_642 a_1 a_733 a_11...       0\n",
       "5           a_719 a_1 a_347 a_340 a_1656 a_885 a_905       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['seq_len'] = data.treatments.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = data[data.target == 0]\n",
    "positive = data[data.target == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((293389, 3), (4440, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative.shape, positive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_examples = defaultdict(list)\n",
    "positive_examples = defaultdict(list)\n",
    "\n",
    "for row in negative.itertuples():\n",
    "    negative_examples[row.seq_len].append(row.treatments.strip())\n",
    "    \n",
    "for row in positive.itertuples():\n",
    "    positive_examples[row.seq_len].append(row.treatments.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictors and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_reader = OneLangSeq2SeqReader(masker=None)\n",
    "seq2seq_vocab = Vocabulary.from_files('vocab_seq2seq_masked')\n",
    "seq2seq_model = get_basic_seq2seq_model(seq2seq_vocab)\n",
    "load_weights(seq2seq_model, 'model_seq2seq_masked.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_reader = InsuranceReader()\n",
    "class_vocab = Vocabulary.from_files('vocab_classification')\n",
    "class_model = get_basic_classification_model(class_vocab)\n",
    "load_weights(class_model, 'model_classification.th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneLanguageSeq2SeqModel(\n",
       "  (_source_embedder): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): Embedding()\n",
       "  )\n",
       "  (_encoder): PytorchSeq2SeqWrapper(\n",
       "    (_module): LSTM(64, 32, batch_first=True)\n",
       "  )\n",
       "  (_attention): AdditiveAttention()\n",
       "  (_target_embedder): Embedding()\n",
       "  (_decoder_cell): LSTMCell(96, 32)\n",
       "  (_output_projection_layer): Linear(in_features=32, out_features=2150, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_model.cuda(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicClassifier(\n",
       "  (_text_field_embedder): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): Embedding()\n",
       "  )\n",
       "  (_seq2seq_encoder): PytorchSeq2SeqWrapper(\n",
       "    (_module): LSTM(32, 16, batch_first=True)\n",
       "  )\n",
       "  (_seq2vec_encoder): BagOfEmbeddingsEncoder()\n",
       "  (_classification_layer): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (_loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_model.cuda(cuda_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WER analysis\n",
    "## positive to negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = BasicIterator(batch_size=1)\n",
    "iterator.index_with(seq2seq_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prob_wer_metrics(splitted_examples, result_path, num_attempts=10):\n",
    "    results = list()\n",
    "    for seq_len, examples in splitted_examples.items():\n",
    "        instances = [seq2seq_reader.text_to_instance(seq) for seq in examples]\n",
    "        for i, example in enumerate(tqdm(iterator(instances, num_epochs=1, shuffle=False))):\n",
    "            example = move_to_device(example, cuda_device)\n",
    "\n",
    "            curr_results = dict()\n",
    "            curr_results['seq_len'] = seq_len\n",
    "            input_seq = examples[i]\n",
    "\n",
    "            no_pert_output = seq2seq_model.forward(**example, random_perturbations=False)\n",
    "            no_pert_decoded = ' '.join(seq2seq_model.decode(no_pert_output)['predicted_tokens'][0])\n",
    "            no_pert_wer = calculate_wer(input_seq, no_pert_decoded)\n",
    "            curr_results['no_pert_wer'] = no_pert_wer\n",
    "\n",
    "            # positive class probability\n",
    "            no_pert_prob = class_model.forward_on_instance(class_reader.text_to_instance(no_pert_decoded))['probs'][1]\n",
    "            curr_results['no_pert_prob'] = no_pert_prob\n",
    "\n",
    "            curr_results['pert_wer'] = list()\n",
    "            curr_results['pert_prob'] = list()\n",
    "            for _ in range(num_attempts):\n",
    "                pert_output = seq2seq_model.forward(**example, random_perturbations=True)\n",
    "                pert_decoded = ' '.join(seq2seq_model.decode(pert_output)['predicted_tokens'][0])\n",
    "                pert_wer = calculate_wer(input_seq, pert_decoded)\n",
    "                curr_results['pert_wer'].append(pert_wer)\n",
    "\n",
    "                pert_prob = class_model.forward_on_instance(class_reader.text_to_instance(pert_decoded))['probs'][1]\n",
    "                curr_results['pert_prob'].append(pert_prob)\n",
    "\n",
    "            with open(result_path, mode='a') as writer:\n",
    "                writer.write(f\"{curr_results}\\n\")\n",
    "\n",
    "            results.append(curr_results)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "338it [00:43,  7.72it/s]\n",
      "313it [00:47,  6.55it/s]\n",
      "301it [00:31,  9.62it/s]\n",
      "88it [00:24,  3.57it/s]\n",
      "269it [00:22, 11.81it/s]\n",
      "133it [00:31,  4.26it/s]\n",
      "311it [00:55,  5.59it/s]\n",
      "183it [00:38,  4.75it/s]\n",
      "156it [00:34,  4.53it/s]\n",
      "312it [00:44,  7.06it/s]\n",
      "316it [00:52,  6.06it/s]\n",
      "123it [00:31,  3.87it/s]\n",
      "198it [00:40,  4.90it/s]\n",
      "329it [00:38,  8.58it/s]\n",
      "283it [00:49,  5.68it/s]\n",
      "284it [00:27, 10.17it/s]\n",
      "243it [00:47,  5.07it/s]\n",
      "121it [00:33,  3.64it/s]\n",
      "139it [00:10, 13.20it/s]\n"
     ]
    }
   ],
   "source": [
    "positive_results = calculate_prob_wer_metrics(positive_examples, 'pos_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11198it [22:31,  7.53it/s]"
     ]
    }
   ],
   "source": [
    "negative_results = calculate_prob_wer_metrics(negative_examples, 'neg_results.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from adat.utils import read_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(logs):\n",
    "    data = []\n",
    "    for log in logs:\n",
    "        results = dict()\n",
    "        results['wer_diff'] = [wer - log['no_pert_wer'] for wer in log['pert_wer']]\n",
    "        results['prob_diff'] = [wer - log['no_pert_prob'] for wer in log['pert_prob']]\n",
    "        for i in range(len(results['prob_diff'])):\n",
    "            curr_res = dict()\n",
    "            curr_res['seq_len'] = log['seq_len']\n",
    "            curr_res['wer_diff'] = results['wer_diff'][i]\n",
    "            curr_res['prob_diff'] = results['prob_diff'][i]\n",
    "            data.append(curr_res)\n",
    "    return = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = get_data(read_logs('positive_results.txt'))\n",
    "negative_data = get_data(read_logs('neg_results.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive examples\n",
    "\n",
    "fig, axes = plt.subplots(4, 5, figsize=(25, 20))\n",
    "\n",
    "for seq_len in range(2, 21):\n",
    "    curr_data = positive_data[positive_data.seq_len == seq_len][['prob_diff', 'wer_diff']]\n",
    "    axes[int((seq_len - 2) / 5), (seq_len - 2) % 5].set_title(f'Seq Len = {seq_len}')\n",
    "    sns.scatterplot(x=\"wer_diff\", y=\"prob_diff\", data=curr_data, ax=axes[int((seq_len - 2) / 5), (seq_len - 2) % 5])\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative examples\n",
    "\n",
    "fig, axes = plt.subplots(4, 5, figsize=(25, 20))\n",
    "\n",
    "for seq_len in range(2, 21):\n",
    "    curr_data = negative_data[negative_data.seq_len == seq_len][['prob_diff', 'wer_diff']]\n",
    "    axes[int((seq_len - 2) / 5), (seq_len - 2) % 5].set_title(f'Seq Len = {seq_len}')\n",
    "    sns.scatterplot(x=\"wer_diff\", y=\"prob_diff\", data=curr_data, ax=axes[int((seq_len - 2) / 5), (seq_len - 2) % 5])\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
